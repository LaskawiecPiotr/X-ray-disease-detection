{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chest X-Ray Multi-label Classification Using Pretrained TorchXRayVision Model\n",
    "#\n",
    "# This script loads a cleaned test dataset of chest X-ray images and uses a \n",
    "# pretrained DenseNet (from torchxrayvision) to perform multi-label classification.\n",
    "#\n",
    "# The workflow is as follows:\n",
    "# 1. Data Loading & Preprocessing:\n",
    "#    - Load test data from a parquet file.\n",
    "#    - Build a lookup dictionary for image paths.\n",
    "# 2. Custom Dataset:\n",
    "#    - Define a PyTorch Dataset to load images and associated labels.\n",
    "# 3. Data Transformations:\n",
    "#    - Apply resizing and other transformations.\n",
    "# 4. Model Loading & Evaluation:\n",
    "#    - Load a pretrained DenseNet from torchxrayvision.\n",
    "#    - Run inference over the test set using an Ignite Engine.\n",
    "# 5. Threshold Optimization:\n",
    "#    - Compute ROC curves for each class and determine an optimal threshold.\n",
    "# 6. Metrics & Visualization:\n",
    "#    - Print classification reports and plot ROC curves.\n",
    "#\n",
    "# Note: Update file paths as necessary.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\piotr\\anaconda3\\Lib\\site-packages\\torchxrayvision\\utils.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x218da3be970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision import models\n",
    "\n",
    "import torchxrayvision as xrv\n",
    "\n",
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import EarlyStopping, ModelCheckpoint, TerminateOnNan\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.metrics import Loss, Accuracy, Precision, Recall\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             precision_score, recall_score, f1_score, roc_curve, auc)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'df_test_cleaned.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m image_path_dict \u001b[38;5;241m=\u001b[39m {os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path): path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m image_paths}\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load the cleaned test dataframe.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_test_cleaned.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'df_test_cleaned.parquet'"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "# Define the root directory for the images.\n",
    "record_path = r\"C:\\Users\\piotr\\.cache\\kagglehub\\datasets\\nih-chest-xrays\\data\\versions\\3\"\n",
    "\n",
    "# Gather all PNG image paths recursively.\n",
    "image_paths = glob.glob(os.path.join(record_path, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "# Build a dictionary mapping image filename to its full path.\n",
    "image_path_dict = {os.path.basename(path): path for path in image_paths}\n",
    "\n",
    "# Load the cleaned test dataframe.\n",
    "df_test = pd.read_parquet(\"df_test_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Dataset for X-Ray Images\n",
    "\n",
    "class XrayDataset(Dataset):\n",
    "    def __init__(self, df, image_path_dict, transform=None):\n",
    "        self.df = df\n",
    "        self.image_path_dict = image_path_dict  # Dictionary for quick lookup\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.df.iloc[idx]['Image Index']\n",
    "        image_path = self.image_path_dict[image_name]\n",
    "        \n",
    "        img = cv2.imread(image_path)\n",
    "        img = xrv.datasets.normalize(img, 255) # convert 8-bit image to [-1024, 1024] range\n",
    "        img = img.mean(2)[None, ...] # Make single color channel\n",
    "        label = self.df.iloc[idx].iloc[1:].values.astype(float)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(label)\n",
    "\n",
    "# Use torchvision.transforms.v2 for consistent transforms.\n",
    "transform = v2.Compose([\n",
    "    # Resize images to 224x224 using XRayResizer provided by torchxrayvision.\n",
    "    xrv.datasets.XRayResizer(224)\n",
    "])\n",
    "\n",
    "# Create the test dataset and DataLoader.\n",
    "test_dataset = XrayDataset(df_test, image_path_dict, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "# List of disease labels from the test dataframe.\n",
    "diseases = df_test.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 682\n",
       "\tepoch: 1\n",
       "\tepoch_length: 682\n",
       "\tmax_epochs: 1\n",
       "\toutput: <class 'tuple'>\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Loading and Evaluation Setup\n",
    "# Set computation device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained DenseNet model from torchxrayvision.\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-nih\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Containers to store predictions and true labels.\n",
    "probs_list = []\n",
    "labels_list = []\n",
    "\n",
    "def evaluate_step(engine, batch):\n",
    "    \"\"\"\n",
    "    Evaluation step for Ignite engine.\n",
    "    Processes a batch of images, computes outputs, and stores probabilities and labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images, label = batch\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Process each sample in the batch.\n",
    "        for i in range(len(outputs)):\n",
    "            # Map model outputs to a dictionary of pathology probabilities.\n",
    "            dict_probs = dict(zip(model.pathologies, outputs[i].cpu().numpy()))\n",
    "            # Extract only the probabilities for the relevant diseases.\n",
    "            probs = [dict_probs[j] for j in diseases]\n",
    "            \n",
    "            # Store predictions and labels.\n",
    "            probs_list.append(probs)\n",
    "            labels_list.append(label[i].cpu().numpy())\n",
    "    \n",
    "    return outputs, label\n",
    "\n",
    "# Create an Ignite evaluator for the test set.\n",
    "evaluator = Engine(evaluate_step)\n",
    "evaluator.run(test_loader)\n",
    "\n",
    "# Convert the stored lists to numpy arrays.\n",
    "probs_array = np.array(probs_list, dtype=object)\n",
    "labels_array = np.array(labels_list, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.5253\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "# Store sum of distances to (0,1) for each threshold\n",
    "sum_distance_per_threshold = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds = (probs_array > threshold).astype(int)  # Apply threshold\n",
    "    total_distance = 0  # Sum of distances across all classes\n",
    "    \n",
    "    for i in range(labels_array.shape[1]):\n",
    "        fpr, tpr, thr = roc_curve(labels_array[:, i].astype(int), probs_array[:, i])\n",
    "        \n",
    "        # Find the closest threshold index\n",
    "        closest_idx = np.argmin(np.abs(thr - threshold))\n",
    "        \n",
    "        # Compute Euclidean distance to (0,1)\n",
    "        distance = np.sqrt((fpr[closest_idx] - 0) ** 2 + (tpr[closest_idx] - 1) ** 2)\n",
    "        total_distance += distance  # Sum distances across classes\n",
    "\n",
    "    sum_distance_per_threshold.append(total_distance)\n",
    "\n",
    "# Find the best threshold\n",
    "best_threshold = thresholds[np.argmin(sum_distance_per_threshold)]\n",
    "print(f\"Optimal Threshold: {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply sigmoid to get probabilities\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m probs_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(probs_list, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m      3\u001b[0m labels_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels_list, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m      4\u001b[0m preds \u001b[38;5;241m=\u001b[39m (probs_array \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply sigmoid to get probabilities\n",
    "probs_array = np.array(probs_list, dtype=object)\n",
    "labels_array = np.array(labels_list, dtype=object)\n",
    "preds = (probs_array > 0.5).astype(int)\n",
    "# %% [markdown]\n",
    "# ## Metrics on Test Set\n",
    "\n",
    "y_true = labels_array.astype(int)\n",
    "y_pred = preds.astype(int)\n",
    "\n",
    "print(\"\\nClassification Report (Per-Class):\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=[df_test.columns.tolist()[1:][i] for i in range(y_true.shape[1])]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Confusion matrix (argmax at the row level can be misleading for multi-label,\n",
    "# but shown here for demonstration.)\n",
    "cm = confusion_matrix(\n",
    "    y_true.argmax(axis=1), \n",
    "    y_pred.argmax(axis=1)\n",
    ")\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ROC Curves (Optional)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(y_true.shape[1]):\n",
    "    fpr, tpr, _ = roc_curve(y_true[:, i], probs_array[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{df_test.columns.tolist()[1:][i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('RUC Curves for the XRayVision pretrained model')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"AUC, the XRayVision pretrained model.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   precision |   recall |   f1-score |\n",
      "|:-------------------|------------:|---------:|-----------:|\n",
      "| Pneumothorax       |   0.118278  | 0.856403 |   0.20785  |\n",
      "| Mass               |   0.0957942 | 0.841717 |   0.172012 |\n",
      "| Atelectasis        |   0.159004  | 0.898336 |   0.270185 |\n",
      "| Nodule             |   0.0785253 | 0.772305 |   0.142556 |\n",
      "| Consolidation      |   0.0842982 | 0.967277 |   0.155081 |\n",
      "| Pleural_Thickening |   0.057971  | 0.842333 |   0.108476 |\n",
      "| Effusion           |   0.256537  | 0.872703 |   0.396515 |\n",
      "| Infiltration       |   0.240613  | 0.92679  |   0.382041 |\n",
      "| micro avg          |   0.13907   | 0.88517  |   0.240375 |\n",
      "| macro avg          |   0.136378  | 0.872233 |   0.22934  |\n",
      "| weighted avg       |   0.173431  | 0.88517  |   0.283441 |\n",
      "| samples avg        |   0.124396  | 0.478369 |   0.186056 |\n"
     ]
    }
   ],
   "source": [
    "report_dict=classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=[df_test.columns.tolist()[1:][i] for i in range(y_true.shape[1])]\n",
    "    ,output_dict=True)\n",
    "df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# Save it as markdown\n",
    "print(df.drop(columns=\"support\").to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
